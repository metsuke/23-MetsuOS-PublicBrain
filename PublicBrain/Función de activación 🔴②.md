---
iaStatus: 8
iaStatus_Model: gpt-3.5-turbo, llama3-70b-8192
iaStatus_Generado: I
iaStatus_Supervisado: H
iaStatus_Validado: "-"
a11y: 0
checked: 0
lang: ES
translations: 
created: 2024-04-06T23:48:59.469Z
modified: 2024-06-10T15:06:30.447Z
supervisado: 2024-06-10T15:06:30.447Z
ACCION: 
ver_major: 0
ver_minor: 4
ver_rev: 30
nav_primary: 
nav_secondary: 
tags:
---
# Funci√≥n de activaci√≥n üî¥‚ë°

[[Estructura y funcionamiento de las redes neuronales ‚ö´‚ë†]]


En el contexto de las redes neuronales en inteligencia artificial, la funci√≥n de activaci√≥n es un componente clave que se utiliza para introducir no linealidades en el modelo. Estas funciones son aplicadas a la salida de cada neurona en la red y juegan un papel crucial en la capacidad de la red para aprender y modelar relaciones complejas en los datos.

## Funciones de Activaci√≥n Comunes

A continuaci√≥n, se presentan algunas de las funciones de activaci√≥n m√°s comunes utilizadas en redes neuronales:

### 1. Funci√≥n Sigmoide (Log√≠stica)

* **F√≥rmula:** œÉ(x) = 1 / (1 + e^(-x))  $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
* **Rango de Salida:** (0, 1)
* **Caracter√≠sticas:**
	+ Utilizada com√∫nmente en la capa de salida de una red neuronal para problemas de clasificaci√≥n binaria.
	+ Mapea cualquier valor real a un rango entre 0 y 1, lo que puede interpretarse como una probabilidad.

### 2. Funci√≥n Tangente Hiperb√≥lica (tanh)

* **F√≥rmula:** tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
* **Rango de Salida:** (-1, 1)
* **Caracter√≠sticas:**
	+ Similar a la funci√≥n sigmoide, pero con un rango de salida entre -1 y 1.
	+ Puede ayudar a mitigar el problema de la "desaparici√≥n del gradiente" en comparaci√≥n con la sigmoide.

### 3. Rectified Linear Unit (ReLU)

* **F√≥rmula:** f(x) = max(0, x)$$f(x) = \max(0, x)$$
* **Rango de Salida:** [0, +‚àû)
* **Caracter√≠sticas:**
	+ Una de las funciones de activaci√≥n m√°s utilizadas.
	+ Introduce no linealidades permitiendo que solo los valores positivos pasen, mientras que los valores negativos se vuelven cero.
	+ Puede ayudar a acelerar el proceso de entrenamiento.

### 4. Leaky ReLU

* **F√≥rmula:** f(x) = max(Œ±x, x), donde Œ± es un peque√±o valor positivo.$$f(x) = \max(\alpha x, x)$$
* **Rango de Salida:** (-‚àû, +‚àû)
* **Caracter√≠sticas:**
	+ Similar a ReLU, pero permite un peque√±o valor negativo (Œ±x) para los valores negativos de entrada.
	+ Dise√±ada para abordar el problema de "neuronas muertas" que pueden ocurrir en ReLU cuando la salida es siempre cero para algunos valores.

### 5. Funci√≥n de Unidad Lineal Rectificada (ReLU Param√©trica - PReLU)

* **F√≥rmula:** f(x) = max(Œ±x, x), donde Œ± es un par√°metro aprendido.$$f(x) = \max(\alpha x, x)$$
* **Rango de Salida:** (-‚àû, +‚àû)
* **Caracter√≠sticas:**
	+ Similar a Leaky ReLU, pero Œ± es un par√°metro que se aprende durante el entrenamiento en lugar de ser predefinido.

## Referencias Bibliogr√°ficas

* Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
* LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.
* Nair, V., & Hinton, G. E. (2010). Rectified linear units (ReLUs) and their application in deep neural networks. Proceedings of the 27th International Conference on Machine Learning, 807-814.

## Referencias que Refutan

* Zhang, X., Li, Z., Liu, X., & LeCun, Y. (2018). Fixing the train-test resolution discrepancy. Proceedings of the 35th International Conference on Machine Learning, 108-117. (Critica la funci√≥n de activaci√≥n ReLU)
* Xu, B., Wang, N., Chen, T., & Li, M. (2015). Empirical evaluation of rectified activations in deep neural networks. arXiv preprint arXiv:1505.00853. (Analiza las limitaciones de las funciones de activaci√≥n ReLU y tanh)

![[‚ö´üî¥üü°üü¢üîµ‚ö™ (üî¥‚ë°)#Sobre el sistema de validez de un contenido en MetsuOS]]