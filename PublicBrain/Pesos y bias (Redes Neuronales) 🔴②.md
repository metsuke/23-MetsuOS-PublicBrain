---
iaStatus: 8
iaStatus_Model: gpt-3.5-turbo, llama3-70b-8192
iaStatus_Generado: "I"
iaStatus_Supervisado: "H"
iaStatus_Validado: "-"
a11y: 0
checked: 0
lang: ES
translations: 
created: 2024-04-06T23:49:00.594Z
modified: 2024-06-24T19:46:59.638Z
supervisado: ""
ACCION: S
ver_major: 0
ver_minor: 2
ver_rev: 29
nav_primary: 
nav_secondary: 
tags:
---
# Pesos y bias (Redes Neuronales) ğŸ”´â‘¡

[[Estructura y funcionamiento de las redes neuronales âš«â‘ ]]

## IntroducciÃ³n

En el contexto de las redes neuronales artificiales, los pesos y los bias son parÃ¡metros fundamentales que permiten a la red aprender y adaptarse a los datos de entrada. Estos parÃ¡metros ajustables son clave para determinar el comportamiento de la red y permiten que Ã©sta haga predicciones o tome decisiones precisas.

## Los Pesos

Los pesos son valores numÃ©ricos asociados a las conexiones entre las neuronas. Cada neurona de una capa estÃ¡ conectada con todas las neuronas de la capa anterior (o de la entrada en el caso de la primera capa). Los pesos determinan la influencia y el nivel de importancia de cada conexiÃ³n en la propagaciÃ³n de la informaciÃ³n a travÃ©s de la red. Al ajustar los pesos, se busca que la red pueda aprender y adaptarse a los datos de entrada de manera Ã³ptima.

## El Bias (Sesgo)

El bias (o sesgo) es un parÃ¡metro adicional asociado a cada neurona, que se suma a la entrada ponderada de dicha neurona antes de aplicar la funciÃ³n de activaciÃ³n. El bias permite ajustar la salida de la neurona en funciÃ³n de una cantidad constante. Esto es Ãºtil cuando los datos de entrada no estÃ¡n centrados alrededor de cero o cuando se necesita introducir algÃºn sesgo en la salida de la neurona.

## Resumen

En resumen, los pesos y los bias son parÃ¡metros ajustables que permiten que una red neuronal aprenda a hacer predicciones o tomar decisiones en funciÃ³n de los datos de entrada. Mediante la adaptaciÃ³n de estos parÃ¡metros, la red busca encontrar un conjunto Ã³ptimo de conexiones y generar salidas que se ajusten a los datos de entrenamiento.

## Referencias BibliogrÃ¡ficas que Apoyan el contenido

* Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press. (CapÃ­tulo 3: Neural Networks)
* Haykin, S. (2009). Neural Networks and Learning Machines. Prentice Hall. (CapÃ­tulo 2: Artificial Neural Networks)

## Referencias BibliogrÃ¡ficas que Refutan el contenido

* Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. ICLR 2017. (Cuestiona la importancia de los pesos y bias en la generalizaciÃ³n de las redes neuronales)
* Raghu, M., Poole, B., Kleinberg, J., Sohl-Dickstein, J., & Ganguli, S. (2017). On the expressive power of deep neural networks. ICML 2017. (Discute la relaciÃ³n entre los pesos y bias y la capacidad de expresiÃ³n de las redes neuronales)


![[âš«ğŸ”´ğŸŸ¡ğŸŸ¢ğŸ”µâšª (ğŸ”´â‘¡)#Sobre el sistema de validez de un contenido en MetsuOS]]
